{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mKK4Rd8vhPS"
   },
   "source": [
    "#  **ICT303 - Assignment 2**\n",
    "\n",
    "**Your name: Caleb Goss**\n",
    "\n",
    "**Student ID: 34090158**\n",
    "\n",
    "**Email: cjmeepers@gmail.com**\n",
    "\n",
    "In this assignment, you will build and train a deep learning model for solving a problem of your choice.\n",
    "\n",
    "\n",
    "You are required to:\n",
    "- Think of a practical problem that you would like to solve. The problem can be related, but not limted to, object detection and recognition from images, text analysis, speech analysis, image unpainting, converting images to artistic painting, action recognition (from images or videos), image to text (i.e., generating textual description for images or videos), or text to image (generating images from text) etc.,\n",
    "- Find an appropriate data set to train and test the model you will develop. Note that the dataset should contain enough data (with groundtruth labels) so that when used for training, the model can generalize well to unseen data.\n",
    "- Design a neural network that will solve the problem\n",
    "- Train the neural network on your training data and then evaluate its performance on test data\n",
    "- Analyze the performance of the network you developed and discuss its limitations.\n",
    "\n",
    "**What to submit:**\n",
    "- A colab notebook that describes:\n",
    " - The problem you would like to solve **[10 Marks]**\n",
    " - The dataset that you will use to train and test the deep learning model that you will develop **[10 marks]**\n",
    " - A diagram that describes the architecture of the neural network that you developed **[10 marks]**\n",
    " - Performance curves - this is includes the loss curves as well as the accuracy **[10 marks]**\n",
    " - A discussion, analysis and justification of the different choices you made and their effect on the performance **[15 marks]**\n",
    " - A discussion, analysis of the limitations of your method. You can also show failure cases and try to understand why did it fail on these cases **[15 marks]**\n",
    "\n",
    "- Source code that runs - this includes both code for training and testing **[30 marks]**\n",
    "\n",
    "You also need to submit the dataset you used for training and testing, or alternatively provide the code that downloads the data.\n",
    "\n",
    "Make sure you reference all sources from which you took information.\n",
    "\n",
    "You are allowed to use existing neural networks (not required to implement them from scratch). But, you must customize the architecture to the problem you want to solve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training an AI to Master 2048: A Deep Q-Learning Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem Description [10 marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Problem:** 2048 is a sliding tile puzzle game played on a 4x4 grid.\n",
    "\n",
    "- **How to Play:** Combine matching number tiles by sliding them in four possible directions (up, down, left, right).\n",
    "\n",
    "- **Objective:** Reach the 2048 tile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Why 2048 is an Interesting AI Problem*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game seems simple at first, but can prove challenging to reach the 2048 tile for several reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Complex Decision Making**\n",
    "    - Each move can affect possibilities many moves later\n",
    "    - What looks like a good move now might lead to game-ending situations\n",
    "    - Players need to maintain organized patterns while adapting to random elements\n",
    "\n",
    "2. **Challenging Learning Environment**\n",
    "    - The 4x4 grid has 4^16 possible states\n",
    "    - Random tile spawns make planning difficult\n",
    "    - Success requires both short-term tactics and a long-term strategy\n",
    "    - Mistakes early in the game might only become apparent much later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why Deep Q-Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After considering various approaches, I chose Deep Q-Learning (DQN) for several reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Natural Fit for the Problem**\n",
    "    - Can handle the continuous state space of the board\n",
    "    - Works well with discrete actions (up, down, left, right)\n",
    "    - Can learn complex patterns through experience\n",
    "\n",
    "2. **Learning from Experience**\n",
    "    - Similar to how humans learn the game\n",
    "    - Improves through trial and error\n",
    "    - Can discover strategies we might not think of\n",
    "\n",
    "3. **Balanced Learning Approach**\n",
    "    - Explores new strategies while exploiting learned patterns\n",
    "    - Can handle delayed rewards (important for 2048)\n",
    "    - Provides clear feedback through Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset/Environment [10 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike traditional machine learning problems, where we start with a dataset, reinforcement learning creates its dataset through an agent's interactions with its environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *State Representation: Making the Game Board Learnable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most crucial decisions was how to represent the game board to the AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_state(self) -> np.ndarray:\n",
    "    state = self._board().flatten()\n",
    "    return np.log2(np.where(state > 0, state, 1)).astype(float)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This code does the following to our state representation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Flattening the Board**\n",
    "    - Converts the 4x4 grid into a 16-element vector\n",
    "    - Preserves all information while simplifying input\n",
    "    - Makes it easier for the neural network to process\n",
    "\n",
    "2. **Log2 Transformation**\n",
    "    - Handles the exponential growth of tile values\n",
    "    - 2048 becomes 11 instead of 2048 (2^11 = 2048)\n",
    "    - Makes learning more stable by reducing value ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Reward Design: Teaching the AI What Matters*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward system is crucial - it's how we tell the AI what \"good\" and \"bad\" moves look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if valid_move:\n",
    "    # Merge reward, use log2 to prevent reward explosion\n",
    "    move_reward = np.log2(move_score) if move_score > 0 else 0\n",
    "\n",
    "    # Reward for achieving new highest tile\n",
    "    new_tile_reward = (np.log2(max_tile) - np.log2(prev_max_tile)\n",
    "                        if max_tile > prev_max_tile else 0)\n",
    "\n",
    "    # Add a small reward for empty spaces\n",
    "    empty_spaces_reward = 0.1 * np.sum(self._board() == 0)\n",
    "\n",
    "    # Reward is the sum of all previous rewards plus a small bonus for each step\n",
    "    reward = move_reward + new_tile_reward + empty_spaces_reward + 0.1\n",
    "else:\n",
    "    # Penalize invalid moves\n",
    "    reward = -2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Each component serves a specific purpose:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Move Reward**\n",
    "    - Encourages combining tiles\n",
    "    - Logarithmic scaling prevents reward explosion\n",
    "    - Immediate feedback for good moves\n",
    "\n",
    "2. **New Tile Reward**\n",
    "    - Bonus for reaching new tile values\n",
    "    - Encourages progression towards 2048\n",
    "\n",
    "3. **Empty Spaces Reward**\n",
    "    - Promotes board management\n",
    "    - Keeps options open for future moves\n",
    "\n",
    "4. **Invalid Move Penalty**\n",
    "    - Discourages trying impossible moves\n",
    "    - Helps learn board boundaries\n",
    "    - Encourages efficient play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Network Architecture [10 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DQN network is what powers the AI decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Core Architecture*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            # First hidden layer\n",
    "            nn.Linear(16, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            # Second hidden layer\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            # Output layer\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's break down why each component matters:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Network Components*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Input Layer (16 neurons)** \n",
    "    - Matches the flattened board size (4x4)\n",
    "    - Each neuron represents one tile position\n",
    "    - Log2-transformed values keep input ranges manageable\n",
    "\n",
    "2. **Hidden Layers (128 neurons each)** \n",
    "    - Two layers provide depth without complexity\n",
    "    - 128 neurons offer enough capacity for pattern recognition\n",
    "    - Consistent size maintains information flow\n",
    "\n",
    "3. **Layer Normalization**\n",
    "    - Stabalizes learning across different board states\n",
    "    - Helps handle varying tile value ranges\n",
    "    - Better than BatchNorm for RL cases (handles individual states better)\n",
    "\n",
    "4. **ReLU Activation**\n",
    "    - Fast, simple, and effective\n",
    "    - Helps learn non-linear patterns\n",
    "    - Prevents vanishing gradient problems\n",
    "\n",
    "5. **Dropout**\n",
    "    - Randomly deactivates neurons during training\n",
    "    - Forces network to learn robust features\n",
    "    - Prevents over-reliance on any single path\n",
    "    - Adds noise and prevents co-adaptation (overfitting)\n",
    "\n",
    "5. **Output Layer (4 neurons)** \n",
    "    - One Q-value for each possible move\n",
    "    - No activation (raw Q-values)\n",
    "    - Directly interpretable as action values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance Curves [10 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Design Decisions & Performance Impacts [15 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Network Architecture*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LayerNorm instead of BatchNorm\n",
    "    - More stable training in reinforcement learning context\n",
    "    - Better handles varying state distributions\n",
    "- Dropouts for regularization\n",
    "    - Prevents overfitting to specific board patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Hyperparameters*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Exploration Strategy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Epsilon Start:** *0.1*\n",
    "    - 10% initial exploration balances learning with competence\n",
    "\n",
    "- **Epsilon End:** *0.001*\n",
    "    - 0.1% final exploration maintains adaptability\n",
    "\n",
    "- **Epsilon Decay:** *0.993*\n",
    "    - Decay rate carefully calculated for 1000 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Memory & Batch Size*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Hidden Layers:** *[128, 128]*\n",
    "    - Small enough to train quickly but deep enough to learn patterns\n",
    "    - Two layers allow for hierarchical feature learning\n",
    "    - 128 neurons provide enough capacity for 2048's state space\n",
    "    - Same size layers maintain consistent representation power\n",
    "- **Batch Size:** *128*\n",
    "    - Large enough for stable gradient updates\n",
    "    - Small enough for efficient computation\n",
    "    - Common power of 2 size of GPU optimization\n",
    "- **Buffer Size:** *100,000*\n",
    "    - Games can be long (100s of moves)\n",
    "    - Need enough diversity in experience\n",
    "    - Stores roughly 500-1000 full games\n",
    "    - Prevents overfitting to recent experience\n",
    "- **Train Episodes:** *1000*\n",
    "    - Enough episodes to learn basic strategies\n",
    "    - Allows for exploration decay\n",
    "    - Can see clear learning trends\n",
    "    - Reasonable training time (10 - 20 minutes)\n",
    "- **Evaluation Episodes:** *20*\n",
    "    - Enough episodes to account for randomness\n",
    "    - Quick enough for frequent evaluation\n",
    "    - Good balance of accuracy vs time\n",
    "- **Evaluation Frequency:** *50*\n",
    "    - Regular checkpoints of progress\n",
    "    - Not too computationally expensive\n",
    "    - Enough episodes to see trends\n",
    "    - Good interval for saving models\n",
    "- **Update Frequency:** *5*\n",
    "    - Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Discussion & Analysis of Limitations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Algorithm Limitations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DQN struggles with long-term planning\n",
    "- Difficulty in connecting early moves to the final outcome\n",
    "- Challenge of exploration in large state space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Performance Bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training efficiency\n",
    "- Exploration vs exploitation balance\n",
    "- Reward sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Source Code [30 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjust Hyperparameters as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS: dict = {\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"EPS_START\": 0.05,\n",
    "    \"EPS_END\": 1e-5,\n",
    "    \"EPS_DECAY\": 0.9999,\n",
    "    \"LR\": 5e-5,\n",
    "    \"HIDDEN_LAYERS\": [128, 128]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_layers: list[int]):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Board size is 4x4 \n",
    "        \n",
    "        # Possible tile values (0, 2, 4, 8, \n",
    "        #                       16, 32, 64, 128, 256, 512,\n",
    "        #                       1024, 2048, 4096, 8192, \n",
    "        #                       16384, 32768)\n",
    "        \n",
    "        # Log2 (2^x) = (0, (0), 1 (2), 2 (4), 3 (8), 4 (16), 5 (32), 6 (64)\n",
    "        #               7 (128), 8 (256), 9 (512), \n",
    "        #               10 (1024), 11 (2048), 12 (4096), \n",
    "        #               13 (8192), 14 (16384), 15 (32768))\n",
    "\n",
    "        self.q_net = self.create_network(input_size, output_size, hidden_layers)\n",
    "\n",
    "    def create_network(\n",
    "        self, input_size: int, output_size: int, hidden_layers: list[int]\n",
    "    ):\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(nn.LayerNorm(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "\n",
    "            in_features = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.q_net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Replay Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\")\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=self.capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "Action = int\n",
    "\n",
    "BATCH_SIZE = HYPERPARAMETERS.get(\"BATCH_SIZE\", 128)\n",
    "MEMORY_CAPACITY = HYPERPARAMETERS.get(\"BUFFER_SIZE\", 100_000)\n",
    "GAMMA = HYPERPARAMETERS.get(\"GAMMA\", 0.99)\n",
    "LEARNING_RATE = HYPERPARAMETERS.get(\"LEARNING_RATE\", 1e-4)\n",
    "HIDDEN_LAYERS = HYPERPARAMETERS.get(\"HIDDEN_LAYERS\", [128, 128])\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int,\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    ):\n",
    "        self.state_size = size * size\n",
    "        self.action_size = 4\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize replay memory and hyperparameters\n",
    "        self.memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "        self.batch_size = BATCH_SIZE \n",
    "        self.gamma = GAMMA # Discount factor\n",
    "        self.lr = LEARNING_RATE\n",
    "        \n",
    "        # Initialize policy and target networks\n",
    "        self.policy_net = DQN(self.state_size, self.action_size, HIDDEN_LAYERS).to(device)\n",
    "        self.target_net = DQN(self.state_size, self.action_size, HIDDEN_LAYERS).to(device)\n",
    "        \n",
    "        # Copy policy network weights to target network\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # Initialize optimizer and loss function\n",
    "        self.optimizer = Adam(self.policy_net.parameters(), lr=self.lr, weight_decay=1e-9)\n",
    "        self.criterion = nn.MSELoss() # Huber loss\n",
    "        \n",
    "    def act(self, state: np.ndarray, epsilon: float, valid_actions: list[int]):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)\n",
    "         \n",
    "        # Filter Q-values for only valid actions and select best action\n",
    "        valid_q_values = q_values[0][valid_actions]\n",
    "        best_action_idx = valid_q_values.argmax().item()\n",
    "        best_action = valid_actions[best_action_idx]\n",
    "    \n",
    "        return best_action\n",
    "            \n",
    "    def optimize_model(self) -> float:\n",
    "        \"\"\"\n",
    "        Perform one step of optimization on the DQN\n",
    "        Returns the loss value\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample and prepare batch from memory buffer\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Create tensors for \n",
    "        state_batch = torch.from_numpy(np.stack(batch.state)).float().to(self.device)\n",
    "        action_batch = torch.tensor(batch.action).long().to(self.device)\n",
    "        reward_batch = torch.tensor(batch.reward).float().to(self.device)\n",
    "        next_state_batch = torch.from_numpy(np.stack(batch.next_state)).float().to(self.device)\n",
    "        done_batch = torch.tensor(batch.done).float().to(self.device)\n",
    "        \n",
    "        # Compute Q(s,a)\n",
    "        q_values = self.policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        \n",
    "        # Compute V(s') for all next states\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_state_batch).max(1)[0]\n",
    "        \n",
    "        # Compute expected Q values\n",
    "        expected_q_values = (next_q_values * self.gamma * (1 - done_batch)) + reward_batch\n",
    "        \n",
    "        # Reset gradients\n",
    "        self.optimizer.zero_grad()\n",
    "            \n",
    "        # Compute loss and optimize\n",
    "        loss = self.criterion(q_values, expected_q_values.unsqueeze(1))\n",
    "        loss.backward()        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update the target network with the policy network's weights\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "    def train(self):\n",
    "        self.policy_net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "UPDATE_FREQUENCY = HYPERPARAMETERS.get(\"UPDATE_FREQ\", 10)\n",
    "EPSILON_START = HYPERPARAMETERS.get(\"EPS_START\", 0.9)\n",
    "EPSILON_END = HYPERPARAMETERS.get(\"EPS_END\", 0.05)\n",
    "EPSILON_DECAY = HYPERPARAMETERS.get(\"EPS_DECAY\", 0.99)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, env, agent: DQNAgent, \n",
    "                 writer: SummaryWriter | None):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.writer = writer\n",
    "        \n",
    "        # self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.agent.optimizer)\n",
    "        \n",
    "        self.max_episodes = MAX_EPISODES\n",
    "        \n",
    "        # Update every 100 steps\n",
    "        self.update_frequency = UPDATE_FREQUENCY\n",
    "        \n",
    "        self.epsilon_start = EPSILON_START\n",
    "        self.epsilon_end = EPSILON_END\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        \n",
    "        self.epsilon = self.epsilon_start\n",
    "\n",
    "    def train(self):\n",
    "        tile_frequencies = defaultdict(int)\n",
    "        \n",
    "        progress_bar = tqdm(range(self.max_episodes), desc=\"Training\")\n",
    "        \n",
    "        for episode in progress_bar:\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            episode_loss = 0\n",
    "            episode_steps = 0\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            self.agent.train()\n",
    "            \n",
    "            while not done:\n",
    "                action = self.agent.act(state, self.epsilon, self.env.get_valid_actions())\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                \n",
    "                self.agent.memory.push(state, action, next_state, reward, done)\n",
    "                \n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "                \n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                episode_loss += self.agent.optimize_model()\n",
    "        \n",
    "                episode_steps += 1\n",
    "                episode_reward += reward\n",
    "                    \n",
    "            episode_loss /= episode_steps\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"reward\": f\"{episode_reward:.2f}\"})\n",
    "            \n",
    "            score = self.env.get_score()\n",
    "            max_tile = self.env.get_max_tile()\n",
    "                \n",
    "            tile_frequencies[max_tile] += 1\n",
    "                \n",
    "            # Log metrics to TensorBoard\n",
    "            self.writer.add_scalar(\"Reward/Train\", episode_reward, episode)\n",
    "            self.writer.add_scalar(\"Score/Train\", score, episode)\n",
    "            self.writer.add_scalar(\"Score/Tile/Train\", max_tile, episode)\n",
    "            self.writer.add_scalar(\"Steps/Train\", episode_steps, episode)\n",
    "            self.writer.add_scalar(\"Loss/Train\", episode_loss, episode)\n",
    "            self.writer.add_scalar(\"Epsilon\", self.epsilon, episode)\n",
    "            \n",
    "            # Log tile frequencies\n",
    "            for tile in [512, 1024, 2048]:\n",
    "                self.writer.add_scalar(f\"Tile/Frequency/{tile}\", tile_frequencies[tile], episode)\n",
    "\n",
    "            self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            if episode % self.update_frequency == 0:\n",
    "                self.agent.update_target_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2048 Game Environment & Logic Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add 2048 Game Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.board[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.board[key] = value\n",
    "\n",
    "    def get_board(self) -> np.ndarray:\n",
    "        return self.board\n",
    "\n",
    "    def set_board(self, board: np.ndarray):\n",
    "        self.board = board\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
    "\n",
    "        self.spawn_tile()\n",
    "        self.spawn_tile()\n",
    "\n",
    "    # Add new tile to the board.\n",
    "    def spawn_tile(self):\n",
    "        # Find all empty cells in board\n",
    "        empty_cells = np.argwhere(self.board == 0)\n",
    "\n",
    "        # If there are empty cells on the board\n",
    "        if empty_cells.size > 0:\n",
    "            i, j = random.choice(empty_cells)\n",
    "            self.board[i, j] = 2 if random.random() < 0.9 else 4\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(\n",
    "            [\" \".join([f\"{tile:4}\" for tile in row]) for row in self.board]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add 2048 Game Board Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_LEFT = 2\n",
    "ACTION_RIGHT = 3\n",
    "\n",
    "ACTION_SET = (ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT)\n",
    "\n",
    "class BoardLogic:\n",
    "    @staticmethod\n",
    "    def move(board: np.ndarray, action: int) -> tuple[np.ndarray, bool, int]:\n",
    "        \"\"\"\n",
    "        Perform a move on the board.\n",
    "\n",
    "        Args:\n",
    "            board (np.ndarray): The current game board.\n",
    "            action (int): The action to perform (UP, DOWN, LEFT, RIGHT).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, bool, int]: New board state, whether move was valid, and score.\n",
    "        \"\"\"\n",
    "        prev_board = board.copy()\n",
    "        new_board = board.copy()\n",
    "        \n",
    "        score: int = 0\n",
    "\n",
    "        def merge(row):\n",
    "            nonlocal score\n",
    "\n",
    "            # Remove zeros and get non-zero values\n",
    "            row = row[row != 0]\n",
    "\n",
    "            # Merge adjacent equal values\n",
    "            for i in range(len(row) - 1):\n",
    "                if row[i] == row[i + 1]:\n",
    "                    row[i] *= 2\n",
    "                    score += row[i]\n",
    "                    row[i + 1] = 0\n",
    "\n",
    "            # Remove zeros again and pad with zeros\n",
    "            row = row[row != 0]\n",
    "            return np.pad(row, (0, 4 - len(row)), \"constant\")\n",
    "\n",
    "        if action == ACTION_UP:\n",
    "            new_board = np.apply_along_axis(merge, 0, new_board)\n",
    "        elif action == ACTION_DOWN:\n",
    "            new_board = np.apply_along_axis(lambda x: merge(x[::-1])[::-1], 0, new_board)\n",
    "        elif action == ACTION_LEFT:\n",
    "            new_board = np.apply_along_axis(merge, 1, new_board)\n",
    "        elif action == ACTION_RIGHT:\n",
    "            new_board = np.apply_along_axis(lambda x: merge(x[::-1])[::-1], 1, new_board)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "\n",
    "        is_valid_move: bool = not np.array_equal(prev_board, new_board)\n",
    "\n",
    "        return new_board, is_valid_move, score\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_similar_tiles(board: np.ndarray) -> int:\n",
    "        count = 0\n",
    "        \n",
    "        for i in range(board.shape[0]):\n",
    "            for j in range(board.shape[1]):\n",
    "                if i < board.shape[0] - 1 and board[i, j] == board[i+1, j] and board[i, j] != 0:\n",
    "                    count += 1\n",
    "                if j < board.shape[1] - 1 and board[i, j] == board[i, j+1] and board[i, j] != 0:\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    @staticmethod\n",
    "    def sum_different_tiles(board: np.ndarray) -> int:\n",
    "        total = 0\n",
    "        \n",
    "        for i in range(board.shape[0]):\n",
    "            for j in range(board.shape[1]):\n",
    "                if i < board.shape[0] - 1:\n",
    "                    total += abs(board[i, j] - board[i+1, j])\n",
    "                if j < board.shape[1] - 1:\n",
    "                    total += abs(board[i, j] - board[i, j+1])\n",
    "                    \n",
    "        return total\n",
    "\n",
    "    @staticmethod\n",
    "    def game_over(board: np.ndarray) -> bool:\n",
    "        # Check for empty spaces\n",
    "        if np.any(board == 0):\n",
    "            return False\n",
    "\n",
    "        # Check for possible vertical or horizontal merges\n",
    "        if np.any(board[:, :-1] == board[:, 1:]) or np.any(board[:-1, :] == board[1:, :]):\n",
    "            return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add 2048 Game Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Abstract environment class (who cares)\n",
    "class IEnv(ABC):\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "\n",
    "class GameEnv(IEnv):\n",
    "    def __init__(self, board_size: int):\n",
    "        self.board_size = board_size\n",
    "        self.board = Board(size=board_size)\n",
    "        \n",
    "        self.score = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.reset()\n",
    "\n",
    "        self.score = 0\n",
    "\n",
    "        return self.get_state()\n",
    "    \n",
    "    def _board(self):\n",
    "        return self.board.get_board()\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        valid_actions = []\n",
    "        \n",
    "        for action in ACTION_SET:\n",
    "            _, valid_move, _ = BoardLogic.move(self._board(), action)\n",
    "            \n",
    "            if valid_move:\n",
    "                valid_actions.append(action)\n",
    "                \n",
    "        return valid_actions\n",
    "\n",
    "    def step(self, action) -> tuple[np.ndarray, float, bool]:\n",
    "        \"\"\"Execute one step in the environment.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The action to take (0: up, 1: right, 2: down, 3: left)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (next_state, reward, done)\n",
    "                - next_state: The new board state after the action\n",
    "                - reward: The reward received for the action\n",
    "                - done: Whether the game is over\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_max_tile = np.max(self._board())\n",
    "        \n",
    "        new_board, valid_move, move_score = BoardLogic.move(self._board(), action)\n",
    "\n",
    "        if valid_move:\n",
    "            self.board.set_board(new_board)\n",
    "            self.board.spawn_tile()\n",
    "            \n",
    "            self.score += move_score\n",
    "            \n",
    "            max_tile = np.max(self._board())\n",
    "            \n",
    "            # Merge reward, use log2 to prevent reward explosion\n",
    "            move_reward = np.log2(move_score) if move_score > 0 else 0\n",
    "            \n",
    "            # Add an expontential reward that scales with the magnitude of improvement\n",
    "            new_tile_reward = (2 ** (np.log2(max_tile) - np.log2(prev_max_tile)) \n",
    "                               if max_tile > prev_max_tile else 0)\n",
    "            \n",
    "            # Add a small reward for empty spaces\n",
    "            empty_spaces_reward = 0.1 * np.sum(self._board() == 0)\n",
    "            \n",
    "            # Reward is the sum of all previous reward plus a small bonus for each step\n",
    "            reward = move_reward + new_tile_reward + empty_spaces_reward + 0.1\n",
    "        else:\n",
    "            # Penalize invalid moves\n",
    "            reward = -2\n",
    "        \n",
    "        done = BoardLogic.game_over(self._board())\n",
    "        \n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        state = self._board().flatten()\n",
    "        return np.log2(np.where(state > 0, state, 1)).astype(int)\n",
    "\n",
    "    def get_score(self) -> int:\n",
    "        return self.score\n",
    "\n",
    "    def get_max_tile(self) -> int:\n",
    "        return np.max(self._board())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SIZE = 4\n",
    "\n",
    "def main() -> int:\n",
    "    env = GameEnv(BOARD_SIZE)\n",
    "    agent = DQNAgent(BOARD_SIZE)\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    trainer = Trainer(env, agent, writer)\n",
    "    trainer.train()\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
